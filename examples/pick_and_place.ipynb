{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Pick and Place Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Robot Ready\n"
     ]
    }
   ],
   "source": [
    "#Imports\n",
    "import numpy as np\n",
    "import open3d  \n",
    "import rmlib\n",
    "\n",
    "rm = rmlib.RMLib()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <u>Lets start by defining a waypoint. There are two ways of saving a waypoint: \n",
    "1. The first way is to save the pose of the tool-tip (TCP) which is stored as an array of [X, Y, Z, rX, rY, rZ]. \n",
    "<br>\n",
    "2. The second way is to store the joint angles of the robot which is also stored as and array with [J1, J2, J3, J4, J5, J6] with Ji is the angle at joint i in radians for a 6-DoF robot.\n",
    "\n",
    "In this example we will store our waypoints as joint angles because it eliminates the possibility of overrotating a joint. \n",
    "<br><br>\n",
    "We will also save the camera transform. The camera transform is a transformation matrix describing the position and orientation of the camera with respect to the base of the robot. We will use the camera transform later to transform objects from a local frame (relative to the camera) to a global frame (relative to the robot base.)\n",
    "<br>\n",
    "#### <b>Position the robot in an orientation that it can see the objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "rm.align_gripper_with_axis()\n",
    "pickup_waypoint = rm.get_joint_angles()\n",
    "\n",
    "# Normalize the wrist joint to give us the largest joint freedom when picking\n",
    "\n",
    "camera_transform = rm.get_camera_transform()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <u>Next lets tune the disparity shift\n",
    "The disparity shift is a camera setting that corrects for the parallax of the two camera lenses. The disparity shift is dependent on the mean height of the point cloud. If the disparity is not set correctly your cloud will be distorted. This particular tuning method sets the disparity based on the estimated distance to the surface. We only need to tune this once because we will be coming back to the same height for each object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disp = rm.tune_disparity_shift()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Disparity shif can also be set maualy if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "disp = rm.set_disparity_shift(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <u>Lets define our drop-off point\n",
    "#### <b>Move the robot to the desired drop point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "rm.align_gripper_with_axis()\n",
    "drop_waypoint = rm.get_joint_angles()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <u> Now lets find some objects!\n",
    "Lets start by getting our point cloud and making some modifications:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move to our initial waypoint.\n",
    "rm.set_joint_angles(pickup_waypoint) \n",
    "# Retrieve point cloud.\n",
    "cloud = rm.get_point_cloud()\n",
    "# Compress the point cloud with a voxel size of 3mm.\n",
    "cloud_vg = rm.downsample_cloud(cloud,leaf_size=0.003)\n",
    "# Remove the table.\n",
    "cloud_vg_nt = rm.remove_planar_surface(cloud_vg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets segment objects pick the highest object, and generate a pose to pick it up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb7c696eff15499680e7bd1edb6b658b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Renderer(camera=PerspectiveCamera(aspect=1.6, fov=90.0, position=(-0.01885669893726679, 0.1712310002454504, 1.…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2440a71c8e64325a41edb5de3c920e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Label(value='Point size:'), FloatSlider(value=0.001, max=0.01, step=1e-05), Label(value='Backgr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "view = rm.PC_Viewer()\n",
    "view.add_cloud(cloud_vg)\n",
    "view.add_cloud(cloud_vg_nt,True)\n",
    "view.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Objects found: 1\n"
     ]
    }
   ],
   "source": [
    "# Segment objects with spreading segmentation algorithm.\n",
    "object_clouds = rm.segment_cloud(cloud_vg_nt,search_radius=0.0033)\n",
    "\n",
    "if len(object_clouds) > 0:\n",
    "    print('Objects found: {}'.format(len(object_clouds)))\n",
    "\n",
    "    # Sort object clouds by height.\n",
    "    object_clouds_sorted = rm.sort_clouds_height(object_clouds)\n",
    "    # Pick the highest cloud as our object.\n",
    "    my_object = object_clouds_sorted[0]\n",
    "\n",
    "    # Find the transformation matrix of this object representing position and orientation.\n",
    "    # The transformation matrix will be positioned on the top of the object with the y axis\n",
    "    # alligned with the principal axis of the object.\n",
    "    object_transform = rm.get_object_transform(my_object,vertical=True)\n",
    "    \n",
    "    object_width = rm.get_object_width(my_object) + 0.01\n",
    "    \n",
    "    # Shift the transform down toward the table.\n",
    "    \n",
    "    ##CHANGE\n",
    "    object_transform = rm.shift_transform_to_grasp(cloud_vg,object_transform,object_width)\n",
    "    # Transform the object transform to a global frame relative to the robot base\n",
    "    global_transform = rm.transform_transform(object_transform,camera_transform)\n",
    "    \n",
    "    # Convert the transformation matrix to a TCP pose.\n",
    "    object_pose = rm.convert_transform_to_pose(global_transform)\n",
    "    # translate the pose up to define another waypoint above the object\n",
    "    object_pose_above = rm.translate_pose(object_pose,0,0,-0.1)\n",
    "else:\n",
    "    print(\"No objects found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <u>Lets view the point cloud to make sure we like the proposed grasp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db442862a6f8444fb491cdba43588afa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Renderer(camera=PerspectiveCamera(aspect=1.6, fov=90.0, position=(-0.01875055725398479, 0.17122910545566866, 1…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2ade04ded3e4b8e90cefbd4fa8a1481",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Label(value='Point size:'), FloatSlider(value=0.001, max=0.01, step=1e-05), Label(value='Backgr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize a viewer object\n",
    "view = rm.PC_Viewer()\n",
    "view.add_cloud(cloud_vg,colorize=True,color=[100,100,100])\n",
    "view.add_cloud(cloud_vg_nt)\n",
    "for object_cloud in object_clouds:\n",
    "    view.add_cloud(object_cloud,colorize=True)\n",
    "view.add_axis(object_transform)\n",
    "\n",
    "# We can also generate boxes representing the gripper given a transform and gripper width\n",
    "gripper_boxes,finger_boxes = rm.get_gripper_boxes(object_transform,object_width)\n",
    "view.add_gripper_boxes(gripper_boxes,finger_boxes)\n",
    "view.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <u>Now lets pick up the object and place it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "rm.set_gripper_width(object_width)\n",
    "\n",
    "# Set the torque low in case the gripper collides with something\n",
    "rm.set_gripper_torque(15)\n",
    "\n",
    "rm.movej(object_pose_above)\n",
    "rm.movel(object_pose)\n",
    "\n",
    "rm.set_gripper_torque(200)\n",
    "time.sleep(0.5)\n",
    "rm.close_gripper()\n",
    "\n",
    "rm.movel(object_pose_above)\n",
    "rm.set_joint_angles(drop_waypoint)\n",
    "\n",
    "rm.open_gripper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
